{
    "metadata": {
        "kernelspec": {
            "name": "python2-spark21", 
            "display_name": "Python 2 with Spark 2.1", 
            "language": "python"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "name": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "file_extension": ".py", 
            "nbconvert_exporter": "python"
        }
    }, 
    "cells": [
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 37, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "metadata": {
                "scrolled": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "root\n |-- PassengerId: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Title: string (nullable = true)\n\nroot\n |-- PassengerId: string (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Title: string (nullable = true)\n\n+-----------+--------+------+------+----+-----+-----+----------------+-------+--------+-------------+\n|PassengerId|Survived|Pclass|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|        Title|\n+-----------+--------+------+------+----+-----+-----+----------------+-------+--------+-------------+\n|          1|     0.0|     3|  male|22.0|  1.0|  0.0|       A/5 21171|   7.25|       S|       Braund|\n|          2|     1.0|     1|female|38.0|  1.0|  0.0|        PC 17599|71.2833|       C|      Cumings|\n|          3|     1.0|     3|female|26.0|  0.0|  0.0|STON/O2. 3101282|  7.925|       S|    Heikkinen|\n|          4|     1.0|     1|female|35.0|  1.0|  0.0|          113803|   53.1|       S|     Futrelle|\n|          5|     0.0|     3|  male|35.0|  0.0|  0.0|          373450|   8.05|       S|        Allen|\n|          7|     0.0|     1|  male|54.0|  0.0|  0.0|           17463|51.8625|       S|     McCarthy|\n|          8|     0.0|     3|  male| 2.0|  3.0|  1.0|          349909| 21.075|       S|      Palsson|\n|          9|     1.0|     3|female|27.0|  0.0|  2.0|          347742|11.1333|       S|      Johnson|\n|         10|     1.0|     2|female|14.0|  1.0|  0.0|          237736|30.0708|       C|       Nasser|\n|         11|     1.0|     3|female| 4.0|  1.0|  1.0|         PP 9549|   16.7|       S|    Sandstrom|\n|         12|     1.0|     1|female|58.0|  0.0|  0.0|          113783|  26.55|       S|      Bonnell|\n|         13|     0.0|     3|  male|20.0|  0.0|  0.0|       A/5. 2151|   8.05|       S|  Saundercock|\n|         14|     0.0|     3|  male|39.0|  1.0|  5.0|          347082| 31.275|       S|    Andersson|\n|         15|     0.0|     3|female|14.0|  0.0|  0.0|          350406| 7.8542|       S|      Vestrom|\n|         16|     1.0|     2|female|55.0|  0.0|  0.0|          248706|   16.0|       S|      Hewlett|\n|         17|     0.0|     3|  male| 2.0|  4.0|  1.0|          382652| 29.125|       Q|         Rice|\n|         19|     0.0|     3|female|31.0|  1.0|  0.0|          345763|   18.0|       S|Vander Planke|\n|         21|     0.0|     2|  male|35.0|  0.0|  0.0|          239865|   26.0|       S|       Fynney|\n|         22|     1.0|     2|  male|34.0|  0.0|  0.0|          248698|   13.0|       S|      Beesley|\n|         23|     1.0|     3|female|15.0|  0.0|  0.0|          330923| 8.0292|       Q|     \"McGowan|\n+-----------+--------+------+------+----+-----+-----+----------------+-------+--------+-------------+\nonly showing top 20 rows\n\n+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+\n|PassengerId|Pclass|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Survived|        Title|\n+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+\n|        892|     3|  male|34.5|  0.0|  0.0|          330911| 7.8292|       Q|     0.0|        Kelly|\n|        893|     3|female|47.0|  1.0|  0.0|          363272|    7.0|       S|     1.0|       Wilkes|\n|        894|     2|  male|62.0|  0.0|  0.0|          240276| 9.6875|       Q|     0.0|        Myles|\n|        895|     3|  male|27.0|  0.0|  0.0|          315154| 8.6625|       S|     0.0|         Wirz|\n|        896|     3|female|22.0|  1.0|  1.0|         3101298|12.2875|       S|     1.0|     Hirvonen|\n|        897|     3|  male|14.0|  0.0|  0.0|            7538|  9.225|       S|     0.0|     Svensson|\n|        898|     3|female|30.0|  0.0|  0.0|          330972| 7.6292|       Q|     1.0|     Connolly|\n|        899|     2|  male|26.0|  1.0|  1.0|          248738|   29.0|       S|     0.0|     Caldwell|\n|        900|     3|female|18.0|  0.0|  0.0|            2657| 7.2292|       C|     1.0|      Abrahim|\n|        901|     3|  male|21.0|  2.0|  0.0|       A/4 48871|  24.15|       S|     0.0|       Davies|\n|        903|     1|  male|46.0|  0.0|  0.0|             694|   26.0|       S|     0.0|        Jones|\n|        904|     1|female|23.0|  1.0|  0.0|           21228|82.2667|       S|     1.0|       Snyder|\n|        905|     2|  male|63.0|  1.0|  0.0|           24065|   26.0|       S|     0.0|       Howard|\n|        906|     1|female|47.0|  1.0|  0.0|     W.E.P. 5734| 61.175|       S|     1.0|      Chaffee|\n|        907|     2|female|24.0|  1.0|  0.0|   SC/PARIS 2167|27.7208|       C|     1.0|    del Carlo|\n|        908|     2|  male|35.0|  0.0|  0.0|          233734|  12.35|       Q|     0.0|        Keane|\n|        909|     3|  male|21.0|  0.0|  0.0|            2692|  7.225|       C|     0.0|        Assaf|\n|        910|     3|female|27.0|  1.0|  0.0|STON/O2. 3101270|  7.925|       S|     1.0|   Ilmakangas|\n|        911|     3|female|45.0|  0.0|  0.0|            2696|  7.225|       C|     1.0|\"Assaf Khalil|\n|        912|     1|  male|55.0|  1.0|  0.0|        PC 17603|   59.4|       C|     0.0|   Rothschild|\n+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 84, 
            "source": "import pandas as pd\n\ndef covertTypes(df):\n    \n    df = (df.withColumn('Age',df['Age'].cast('double'))\n            .withColumn('SibSp',df['SibSp'].cast('double'))\n            .withColumn('Parch',df['Parch'].cast('double'))\n            .withColumn('Fare',df['Fare'].cast('double'))\n            .withColumn('Survived',df['Survived'].cast('double')))\n    return df \ndf = covertTypes(df)\ntest_df = covertTypes(test_df)\n\ndf.printSchema()\ntest_df.printSchema()\n\ndf.show()\ntest_df.show()"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 43, 
            "source": "df = df.drop('Cabin')\ntest_df = test_df.drop('Cabin')"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "root\n |-- PassengerId: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Title: string (nullable = true)\n\nroot\n |-- PassengerId: string (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Title: string (nullable = true)\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 58, 
            "source": "df.printSchema()\ntest_df.printSchema()"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 59, 
            "source": "df = df.na.drop()\ntest_df = test_df.na.drop()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "root\n |-- PassengerId: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Title: string (nullable = true)\n\nroot\n |-- PassengerId: string (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Title: string (nullable = true)\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 85, 
            "source": "df.printSchema()\ntest_df.printSchema()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-61-bf3d44c13310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#def dropName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgetTitleTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetTitleTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \"\"\"\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mAnalysisException\u001b[0m: u'Cannot resolve column name \"Name\" among (PassengerId, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Embarked, Survived, Title);'"
                    ], 
                    "ename": "AnalysisException", 
                    "evalue": "u'Cannot resolve column name \"Name\" among (PassengerId, Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Embarked, Survived, Title);'"
                }
            ], 
            "execution_count": 61, 
            "source": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n \n## created user defined function to extract title\n#def dropName\ngetTitleTest = udf(lambda name: name.split(',')[0].strip(),StringType())\ntest_df = test_df.withColumn('Title', getTitleTest(test_df['Name']))\ntest_df = test_df.drop('Name')\n\n\ngetTitle = udf(lambda name: name.split(',')[0].strip(),StringType())\ndf = df.withColumn('Title', getTitle(df['Name']))\ndf = df.drop('Name')\n#return df;\n\n#df = dropNames(df)\n#test_df = dropNames(test_df)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 65, 
            "source": "###One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"PassengerId\",\"Title\",\"Pclass\", \"Sex\", \"Ticket\", \"Embarked\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer,encoder]\n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+\n|PassengerId|Pclass|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Survived|        Title|\n+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+\n|        892|     3|  male|34.5|  0.0|  0.0|          330911| 7.8292|       Q|     0.0|        Kelly|\n|        893|     3|female|47.0|  1.0|  0.0|          363272|    7.0|       S|     1.0|       Wilkes|\n|        894|     2|  male|62.0|  0.0|  0.0|          240276| 9.6875|       Q|     0.0|        Myles|\n|        895|     3|  male|27.0|  0.0|  0.0|          315154| 8.6625|       S|     0.0|         Wirz|\n|        896|     3|female|22.0|  1.0|  1.0|         3101298|12.2875|       S|     1.0|     Hirvonen|\n|        897|     3|  male|14.0|  0.0|  0.0|            7538|  9.225|       S|     0.0|     Svensson|\n|        898|     3|female|30.0|  0.0|  0.0|          330972| 7.6292|       Q|     1.0|     Connolly|\n|        899|     2|  male|26.0|  1.0|  1.0|          248738|   29.0|       S|     0.0|     Caldwell|\n|        900|     3|female|18.0|  0.0|  0.0|            2657| 7.2292|       C|     1.0|      Abrahim|\n|        901|     3|  male|21.0|  2.0|  0.0|       A/4 48871|  24.15|       S|     0.0|       Davies|\n|        903|     1|  male|46.0|  0.0|  0.0|             694|   26.0|       S|     0.0|        Jones|\n|        904|     1|female|23.0|  1.0|  0.0|           21228|82.2667|       S|     1.0|       Snyder|\n|        905|     2|  male|63.0|  1.0|  0.0|           24065|   26.0|       S|     0.0|       Howard|\n|        906|     1|female|47.0|  1.0|  0.0|     W.E.P. 5734| 61.175|       S|     1.0|      Chaffee|\n|        907|     2|female|24.0|  1.0|  0.0|   SC/PARIS 2167|27.7208|       C|     1.0|    del Carlo|\n|        908|     2|  male|35.0|  0.0|  0.0|          233734|  12.35|       Q|     0.0|        Keane|\n|        909|     3|  male|21.0|  0.0|  0.0|            2692|  7.225|       C|     0.0|        Assaf|\n|        910|     3|female|27.0|  1.0|  0.0|STON/O2. 3101270|  7.925|       S|     1.0|   Ilmakangas|\n|        911|     3|female|45.0|  0.0|  0.0|            2696|  7.225|       C|     1.0|\"Assaf Khalil|\n|        912|     1|  male|55.0|  1.0|  0.0|        PC 17603|   59.4|       C|     0.0|   Rothschild|\n+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 66, 
            "source": "\ntest_df.show()"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 67, 
            "source": "# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"Survived\", outputCol = \"label\")\nstages += [label_stringIdx]"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 68, 
            "source": "# Transform all features into a vector using VectorAssembler\nnumericCols = [\"Age\", \"SibSp\",\"Parch\", \"Fare\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "root\n |-- PassengerId: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Title: string (nullable = true)\n\nroot\n |-- PassengerId: string (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Title: string (nullable = true)\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 69, 
            "source": "df.printSchema()\ntest_df.printSchema()"
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+----------------+-------------------+----------+-----------------+-----------+--------------+--------+-------------+-----------+-----------------+-------------+----------------+-----+--------------------+\n|PassengerId|Pclass|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Survived|        Title|PassengerIdIndex|PassengerIdclassVec|TitleIndex|    TitleclassVec|PclassIndex|PclassclassVec|SexIndex|  SexclassVec|TicketIndex|   TicketclassVec|EmbarkedIndex|EmbarkedclassVec|label|            features|\n+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+----------------+-------------------+----------+-----------------+-----------+--------------+--------+-------------+-----------+-----------------+-------------+----------------+-----+--------------------+\n|        892|     3|  male|34.5|  0.0|  0.0|          330911| 7.8292|       Q|     0.0|        Kelly|            59.0|   (330,[59],[1.0])|     241.0|(282,[241],[1.0])|        0.0| (2,[0],[1.0])|     0.0|(1,[0],[1.0])|      200.0|(283,[200],[1.0])|          2.0|       (2,[],[])|  0.0|(904,[59,571,612,...|\n|        893|     3|female|47.0|  1.0|  0.0|          363272|    7.0|       S|     1.0|       Wilkes|           302.0|  (330,[302],[1.0])|      50.0| (282,[50],[1.0])|        0.0| (2,[0],[1.0])|     1.0|    (1,[],[])|      113.0|(283,[113],[1.0])|          0.0|   (2,[0],[1.0])|  1.0|(904,[302,380,612...|\n|        894|     2|  male|62.0|  0.0|  0.0|          240276| 9.6875|       Q|     0.0|        Myles|           242.0|  (330,[242],[1.0])|     271.0|(282,[271],[1.0])|        2.0|     (2,[],[])|     0.0|(1,[0],[1.0])|      149.0|(283,[149],[1.0])|          2.0|       (2,[],[])|  0.0|(904,[242,601,614...|\n|        895|     3|  male|27.0|  0.0|  0.0|          315154| 8.6625|       S|     0.0|         Wirz|            95.0|   (330,[95],[1.0])|     212.0|(282,[212],[1.0])|        0.0| (2,[0],[1.0])|     0.0|(1,[0],[1.0])|      263.0|(283,[263],[1.0])|          0.0|   (2,[0],[1.0])|  0.0|(904,[95,542,612,...|\n|        896|     3|female|22.0|  1.0|  1.0|         3101298|12.2875|       S|     1.0|     Hirvonen|            26.0|   (330,[26],[1.0])|     244.0|(282,[244],[1.0])|        0.0| (2,[0],[1.0])|     1.0|    (1,[],[])|      151.0|(283,[151],[1.0])|          0.0|   (2,[0],[1.0])|  1.0|(904,[26,574,612,...|\n|        897|     3|  male|14.0|  0.0|  0.0|            7538|  9.225|       S|     0.0|     Svensson|           283.0|  (330,[283],[1.0])|      69.0| (282,[69],[1.0])|        0.0| (2,[0],[1.0])|     0.0|(1,[0],[1.0])|      266.0|(283,[266],[1.0])|          0.0|   (2,[0],[1.0])|  0.0|(904,[283,399,612...|\n|        898|     3|female|30.0|  0.0|  0.0|          330972| 7.6292|       Q|     1.0|     Connolly|           192.0|  (330,[192],[1.0])|     185.0|(282,[185],[1.0])|        0.0| (2,[0],[1.0])|     1.0|    (1,[],[])|      190.0|(283,[190],[1.0])|          2.0|       (2,[],[])|  1.0|(904,[192,515,612...|\n|        899|     2|  male|26.0|  1.0|  1.0|          248738|   29.0|       S|     0.0|     Caldwell|           111.0|  (330,[111],[1.0])|     259.0|(282,[259],[1.0])|        2.0|     (2,[],[])|     0.0|(1,[0],[1.0])|      160.0|(283,[160],[1.0])|          0.0|   (2,[0],[1.0])|  0.0|(904,[111,589,614...|\n|        900|     3|female|18.0|  0.0|  0.0|            2657| 7.2292|       C|     1.0|      Abrahim|           135.0|  (330,[135],[1.0])|     173.0|(282,[173],[1.0])|        0.0| (2,[0],[1.0])|     1.0|    (1,[],[])|       71.0| (283,[71],[1.0])|          1.0|   (2,[1],[1.0])|  1.0|(904,[135,503,612...|\n|        901|     3|  male|21.0|  2.0|  0.0|       A/4 48871|  24.15|       S|     0.0|       Davies|           268.0|  (330,[268],[1.0])|       0.0|  (282,[0],[1.0])|        0.0| (2,[0],[1.0])|     0.0|(1,[0],[1.0])|      236.0|(283,[236],[1.0])|          0.0|   (2,[0],[1.0])|  0.0|(904,[268,330,612...|\n|        903|     1|  male|46.0|  0.0|  0.0|             694|   26.0|       S|     0.0|        Jones|           117.0|  (330,[117],[1.0])|     141.0|(282,[141],[1.0])|        1.0| (2,[1],[1.0])|     0.0|(1,[0],[1.0])|      203.0|(283,[203],[1.0])|          0.0|   (2,[0],[1.0])|  0.0|(904,[117,471,613...|\n|        904|     1|female|23.0|  1.0|  0.0|           21228|82.2667|       S|     1.0|       Snyder|            69.0|   (330,[69],[1.0])|      34.0| (282,[34],[1.0])|        1.0| (2,[1],[1.0])|     1.0|    (1,[],[])|       34.0| (283,[34],[1.0])|          0.0|   (2,[0],[1.0])|  1.0|(904,[69,364,613,...|\n|        905|     2|  male|63.0|  1.0|  0.0|           24065|   26.0|       S|     0.0|       Howard|           287.0|  (330,[287],[1.0])|      17.0| (282,[17],[1.0])|        2.0|     (2,[],[])|     0.0|(1,[0],[1.0])|       19.0| (283,[19],[1.0])|          0.0|   (2,[0],[1.0])|  0.0|(904,[287,347,614...|\n|        906|     1|female|47.0|  1.0|  0.0|     W.E.P. 5734| 61.175|       S|     1.0|      Chaffee|           198.0|  (330,[198],[1.0])|      86.0| (282,[86],[1.0])|        1.0| (2,[1],[1.0])|     1.0|    (1,[],[])|      219.0|(283,[219],[1.0])|          0.0|   (2,[0],[1.0])|  1.0|(904,[198,416,613...|\n|        907|     2|female|24.0|  1.0|  0.0|   SC/PARIS 2167|27.7208|       C|     1.0|    del Carlo|            99.0|   (330,[99],[1.0])|     255.0|(282,[255],[1.0])|        2.0|     (2,[],[])|     1.0|    (1,[],[])|       74.0| (283,[74],[1.0])|          1.0|   (2,[1],[1.0])|  1.0|(904,[99,585,689,...|\n|        908|     2|  male|35.0|  0.0|  0.0|          233734|  12.35|       Q|     0.0|        Keane|            70.0|   (330,[70],[1.0])|     106.0|(282,[106],[1.0])|        2.0|     (2,[],[])|     0.0|(1,[0],[1.0])|       49.0| (283,[49],[1.0])|          2.0|       (2,[],[])|  0.0|(904,[70,436,614,...|\n|        909|     3|  male|21.0|  0.0|  0.0|            2692|  7.225|       C|     0.0|        Assaf|           155.0|  (330,[155],[1.0])|     104.0|(282,[104],[1.0])|        0.0| (2,[0],[1.0])|     0.0|(1,[0],[1.0])|      260.0|(283,[260],[1.0])|          1.0|   (2,[1],[1.0])|  0.0|(904,[155,434,612...|\n|        910|     3|female|27.0|  1.0|  0.0|STON/O2. 3101270|  7.925|       S|     1.0|   Ilmakangas|           168.0|  (330,[168],[1.0])|     237.0|(282,[237],[1.0])|        0.0| (2,[0],[1.0])|     1.0|    (1,[],[])|      240.0|(283,[240],[1.0])|          0.0|   (2,[0],[1.0])|  1.0|(904,[168,567,612...|\n|        911|     3|female|45.0|  0.0|  0.0|            2696|  7.225|       C|     1.0|\"Assaf Khalil|           286.0|  (330,[286],[1.0])|     263.0|(282,[263],[1.0])|        0.0| (2,[0],[1.0])|     1.0|    (1,[],[])|       62.0| (283,[62],[1.0])|          1.0|   (2,[1],[1.0])|  1.0|(904,[286,593,612...|\n|        912|     1|  male|55.0|  1.0|  0.0|        PC 17603|   59.4|       C|     0.0|   Rothschild|           204.0|  (330,[204],[1.0])|     168.0|(282,[168],[1.0])|        1.0| (2,[1],[1.0])|     0.0|(1,[0],[1.0])|      231.0|(283,[231],[1.0])|          1.0|   (2,[1],[1.0])|  0.0|(904,[204,498,613...|\n+-----------+------+------+----+-----+-----+----------------+-------+--------+--------+-------------+----------------+-------------------+----------+-----------------+-----------+--------------+--------+-------------+-----------+-----------------+-------------+----------------+-----+--------------------+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 71, 
            "source": "cols = test_df.columns\n# Create a Pipeline.\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\n\npipeline = Pipeline(stages=stages)\ndef fitDf(df):\n    \n    pipelineModel = pipeline.fit(df)\n    dataset1 = pipelineModel.transform(df)\n    return dataset1\n\ndataset = fitDf(df)\ntestdataSet = fitDf(test_df)\n\ntestdataSet.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 72, 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>features</th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>Survived</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>892</td>\n      <td>3</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>Q</td>\n      <td>0</td>\n      <td>Kelly</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>893</td>\n      <td>3</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Wilkes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>894</td>\n      <td>2</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>Q</td>\n      <td>0</td>\n      <td>Myles</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>895</td>\n      <td>3</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Wirz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>896</td>\n      <td>3</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Hirvonen</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>897</td>\n      <td>3</td>\n      <td>male</td>\n      <td>14.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7538</td>\n      <td>9.2250</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Svensson</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>898</td>\n      <td>3</td>\n      <td>female</td>\n      <td>30.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330972</td>\n      <td>7.6292</td>\n      <td>Q</td>\n      <td>1</td>\n      <td>Connolly</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>899</td>\n      <td>2</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>248738</td>\n      <td>29.0000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Caldwell</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>900</td>\n      <td>3</td>\n      <td>female</td>\n      <td>18.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2657</td>\n      <td>7.2292</td>\n      <td>C</td>\n      <td>1</td>\n      <td>Abrahim</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>901</td>\n      <td>3</td>\n      <td>male</td>\n      <td>21.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>A/4 48871</td>\n      <td>24.1500</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Davies</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>903</td>\n      <td>1</td>\n      <td>male</td>\n      <td>46.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>694</td>\n      <td>26.0000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Jones</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>904</td>\n      <td>1</td>\n      <td>female</td>\n      <td>23.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>21228</td>\n      <td>82.2667</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Snyder</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>905</td>\n      <td>2</td>\n      <td>male</td>\n      <td>63.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>24065</td>\n      <td>26.0000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Howard</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>906</td>\n      <td>1</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>W.E.P. 5734</td>\n      <td>61.1750</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Chaffee</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>907</td>\n      <td>2</td>\n      <td>female</td>\n      <td>24.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>SC/PARIS 2167</td>\n      <td>27.7208</td>\n      <td>C</td>\n      <td>1</td>\n      <td>del Carlo</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>908</td>\n      <td>2</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>233734</td>\n      <td>12.3500</td>\n      <td>Q</td>\n      <td>0</td>\n      <td>Keane</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>909</td>\n      <td>3</td>\n      <td>male</td>\n      <td>21.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2692</td>\n      <td>7.2250</td>\n      <td>C</td>\n      <td>0</td>\n      <td>Assaf</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>910</td>\n      <td>3</td>\n      <td>female</td>\n      <td>27.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>STON/O2. 3101270</td>\n      <td>7.9250</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Ilmakangas</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>911</td>\n      <td>3</td>\n      <td>female</td>\n      <td>45.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2696</td>\n      <td>7.2250</td>\n      <td>C</td>\n      <td>1</td>\n      <td>\"Assaf Khalil</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>912</td>\n      <td>1</td>\n      <td>male</td>\n      <td>55.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17603</td>\n      <td>59.4000</td>\n      <td>C</td>\n      <td>0</td>\n      <td>Rothschild</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>913</td>\n      <td>3</td>\n      <td>male</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>C 17368</td>\n      <td>3.1708</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Olsen</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>915</td>\n      <td>1</td>\n      <td>male</td>\n      <td>21.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>PC 17597</td>\n      <td>61.3792</td>\n      <td>C</td>\n      <td>0</td>\n      <td>Williams</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>916</td>\n      <td>1</td>\n      <td>female</td>\n      <td>48.0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>PC 17608</td>\n      <td>262.3750</td>\n      <td>C</td>\n      <td>1</td>\n      <td>Ryerson</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>917</td>\n      <td>3</td>\n      <td>male</td>\n      <td>50.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5. 3337</td>\n      <td>14.5000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Robins</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>918</td>\n      <td>1</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>113509</td>\n      <td>61.9792</td>\n      <td>C</td>\n      <td>1</td>\n      <td>Ostby</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>919</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2698</td>\n      <td>7.2250</td>\n      <td>C</td>\n      <td>0</td>\n      <td>Daher</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>920</td>\n      <td>1</td>\n      <td>male</td>\n      <td>41.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113054</td>\n      <td>30.5000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Brady</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>922</td>\n      <td>2</td>\n      <td>male</td>\n      <td>50.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>SC/AH 3085</td>\n      <td>26.0000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Louch</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>923</td>\n      <td>2</td>\n      <td>male</td>\n      <td>24.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>C.A. 31029</td>\n      <td>31.5000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Jefferys</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>924</td>\n      <td>3</td>\n      <td>female</td>\n      <td>33.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>C.A. 2315</td>\n      <td>20.5750</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Dean</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>301</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1273</td>\n      <td>3</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330910</td>\n      <td>7.8792</td>\n      <td>Q</td>\n      <td>0</td>\n      <td>Foley</td>\n    </tr>\n    <tr>\n      <th>302</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1275</td>\n      <td>3</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>376566</td>\n      <td>16.1000</td>\n      <td>S</td>\n      <td>1</td>\n      <td>McNamee</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1277</td>\n      <td>2</td>\n      <td>female</td>\n      <td>24.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>220845</td>\n      <td>65.0000</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Herman</td>\n    </tr>\n    <tr>\n      <th>304</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n      <td>1278</td>\n      <td>3</td>\n      <td>male</td>\n      <td>24.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>349911</td>\n      <td>7.7750</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Aronsson</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1279</td>\n      <td>2</td>\n      <td>male</td>\n      <td>57.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>244346</td>\n      <td>13.0000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Ashby</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1280</td>\n      <td>3</td>\n      <td>male</td>\n      <td>21.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>364858</td>\n      <td>7.7500</td>\n      <td>Q</td>\n      <td>0</td>\n      <td>Canavan</td>\n    </tr>\n    <tr>\n      <th>307</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1281</td>\n      <td>3</td>\n      <td>male</td>\n      <td>6.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Palsson</td>\n    </tr>\n    <tr>\n      <th>308</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1282</td>\n      <td>1</td>\n      <td>male</td>\n      <td>23.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12749</td>\n      <td>93.5000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Payne</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1283</td>\n      <td>1</td>\n      <td>female</td>\n      <td>51.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>PC 17592</td>\n      <td>39.4000</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Lines</td>\n    </tr>\n    <tr>\n      <th>310</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1284</td>\n      <td>3</td>\n      <td>male</td>\n      <td>13.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>C.A. 2673</td>\n      <td>20.2500</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Abbott</td>\n    </tr>\n    <tr>\n      <th>311</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1285</td>\n      <td>2</td>\n      <td>male</td>\n      <td>47.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>C.A. 30769</td>\n      <td>10.5000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Gilbert</td>\n    </tr>\n    <tr>\n      <th>312</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1286</td>\n      <td>3</td>\n      <td>male</td>\n      <td>29.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>315153</td>\n      <td>22.0250</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Kink-Heilmann</td>\n    </tr>\n    <tr>\n      <th>313</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1287</td>\n      <td>1</td>\n      <td>female</td>\n      <td>18.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>13695</td>\n      <td>60.0000</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Smith</td>\n    </tr>\n    <tr>\n      <th>314</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1288</td>\n      <td>3</td>\n      <td>male</td>\n      <td>24.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>371109</td>\n      <td>7.2500</td>\n      <td>Q</td>\n      <td>0</td>\n      <td>Colbert</td>\n    </tr>\n    <tr>\n      <th>315</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1289</td>\n      <td>1</td>\n      <td>female</td>\n      <td>48.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13567</td>\n      <td>79.2000</td>\n      <td>C</td>\n      <td>1</td>\n      <td>Frolicher-Stehli</td>\n    </tr>\n    <tr>\n      <th>316</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1290</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>347065</td>\n      <td>7.7750</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Larsson-Rondberg</td>\n    </tr>\n    <tr>\n      <th>317</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1291</td>\n      <td>3</td>\n      <td>male</td>\n      <td>31.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21332</td>\n      <td>7.7333</td>\n      <td>Q</td>\n      <td>0</td>\n      <td>Conlon</td>\n    </tr>\n    <tr>\n      <th>318</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1292</td>\n      <td>1</td>\n      <td>female</td>\n      <td>30.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36928</td>\n      <td>164.8667</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Bonnell</td>\n    </tr>\n    <tr>\n      <th>319</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1293</td>\n      <td>2</td>\n      <td>male</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28664</td>\n      <td>21.0000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Gale</td>\n    </tr>\n    <tr>\n      <th>320</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1294</td>\n      <td>1</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112378</td>\n      <td>59.4000</td>\n      <td>C</td>\n      <td>1</td>\n      <td>Gibson</td>\n    </tr>\n    <tr>\n      <th>321</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1295</td>\n      <td>1</td>\n      <td>male</td>\n      <td>17.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113059</td>\n      <td>47.1000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Carrau</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1296</td>\n      <td>1</td>\n      <td>male</td>\n      <td>43.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>17765</td>\n      <td>27.7208</td>\n      <td>C</td>\n      <td>0</td>\n      <td>Frauenthal</td>\n    </tr>\n    <tr>\n      <th>323</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1297</td>\n      <td>2</td>\n      <td>male</td>\n      <td>20.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>SC/PARIS 2166</td>\n      <td>13.8625</td>\n      <td>C</td>\n      <td>0</td>\n      <td>\"Nourney</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1298</td>\n      <td>2</td>\n      <td>male</td>\n      <td>23.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28666</td>\n      <td>10.5000</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Ware</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1299</td>\n      <td>1</td>\n      <td>male</td>\n      <td>50.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>113503</td>\n      <td>211.5000</td>\n      <td>C</td>\n      <td>0</td>\n      <td>Widener</td>\n    </tr>\n    <tr>\n      <th>326</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1301</td>\n      <td>3</td>\n      <td>female</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>SOTON/O.Q. 3101315</td>\n      <td>13.7750</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Peacock</td>\n    </tr>\n    <tr>\n      <th>327</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1303</td>\n      <td>1</td>\n      <td>female</td>\n      <td>37.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19928</td>\n      <td>90.0000</td>\n      <td>Q</td>\n      <td>1</td>\n      <td>Minahan</td>\n    </tr>\n    <tr>\n      <th>328</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1304</td>\n      <td>3</td>\n      <td>female</td>\n      <td>28.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>347086</td>\n      <td>7.7750</td>\n      <td>S</td>\n      <td>1</td>\n      <td>Henriksson</td>\n    </tr>\n    <tr>\n      <th>329</th>\n      <td>1</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1306</td>\n      <td>1</td>\n      <td>female</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>PC 17758</td>\n      <td>108.9000</td>\n      <td>C</td>\n      <td>1</td>\n      <td>Oliva y Ocana</td>\n    </tr>\n    <tr>\n      <th>330</th>\n      <td>0</td>\n      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>1307</td>\n      <td>3</td>\n      <td>male</td>\n      <td>38.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>SOTON/O.Q. 3101262</td>\n      <td>7.2500</td>\n      <td>S</td>\n      <td>0</td>\n      <td>Saether</td>\n    </tr>\n  </tbody>\n</table>\n<p>331 rows \u00d7 13 columns</p>\n</div>", 
                        "text/plain": "     label                                           features PassengerId  \\\n0        0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         892   \n1        1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         893   \n2        0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         894   \n3        0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         895   \n4        1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         896   \n5        0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         897   \n6        1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         898   \n7        0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         899   \n8        1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         900   \n9        0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         901   \n10       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         903   \n11       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         904   \n12       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         905   \n13       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         906   \n14       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         907   \n15       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         908   \n16       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         909   \n17       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         910   \n18       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         911   \n19       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         912   \n20       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         913   \n21       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         915   \n22       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         916   \n23       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         917   \n24       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         918   \n25       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         919   \n26       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         920   \n27       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         922   \n28       0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         923   \n29       1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         924   \n..     ...                                                ...         ...   \n301      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1273   \n302      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1275   \n303      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1277   \n304      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...        1278   \n305      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1279   \n306      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1280   \n307      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1281   \n308      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1282   \n309      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1283   \n310      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1284   \n311      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1285   \n312      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1286   \n313      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1287   \n314      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1288   \n315      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1289   \n316      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1290   \n317      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1291   \n318      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1292   \n319      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1293   \n320      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1294   \n321      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1295   \n322      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1296   \n323      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1297   \n324      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1298   \n325      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1299   \n326      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1301   \n327      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1303   \n328      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1304   \n329      1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1306   \n330      0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        1307   \n\n    Pclass     Sex   Age  SibSp  Parch              Ticket      Fare Embarked  \\\n0        3    male  34.5      0      0              330911    7.8292        Q   \n1        3  female  47.0      1      0              363272    7.0000        S   \n2        2    male  62.0      0      0              240276    9.6875        Q   \n3        3    male  27.0      0      0              315154    8.6625        S   \n4        3  female  22.0      1      1             3101298   12.2875        S   \n5        3    male  14.0      0      0                7538    9.2250        S   \n6        3  female  30.0      0      0              330972    7.6292        Q   \n7        2    male  26.0      1      1              248738   29.0000        S   \n8        3  female  18.0      0      0                2657    7.2292        C   \n9        3    male  21.0      2      0           A/4 48871   24.1500        S   \n10       1    male  46.0      0      0                 694   26.0000        S   \n11       1  female  23.0      1      0               21228   82.2667        S   \n12       2    male  63.0      1      0               24065   26.0000        S   \n13       1  female  47.0      1      0         W.E.P. 5734   61.1750        S   \n14       2  female  24.0      1      0       SC/PARIS 2167   27.7208        C   \n15       2    male  35.0      0      0              233734   12.3500        Q   \n16       3    male  21.0      0      0                2692    7.2250        C   \n17       3  female  27.0      1      0    STON/O2. 3101270    7.9250        S   \n18       3  female  45.0      0      0                2696    7.2250        C   \n19       1    male  55.0      1      0            PC 17603   59.4000        C   \n20       3    male   9.0      0      1             C 17368    3.1708        S   \n21       1    male  21.0      0      1            PC 17597   61.3792        C   \n22       1  female  48.0      1      3            PC 17608  262.3750        C   \n23       3    male  50.0      1      0           A/5. 3337   14.5000        S   \n24       1  female  22.0      0      1              113509   61.9792        C   \n25       3    male  22.5      0      0                2698    7.2250        C   \n26       1    male  41.0      0      0              113054   30.5000        S   \n27       2    male  50.0      1      0          SC/AH 3085   26.0000        S   \n28       2    male  24.0      2      0          C.A. 31029   31.5000        S   \n29       3  female  33.0      1      2           C.A. 2315   20.5750        S   \n..     ...     ...   ...    ...    ...                 ...       ...      ...   \n301      3    male  26.0      0      0              330910    7.8792        Q   \n302      3  female  19.0      1      0              376566   16.1000        S   \n303      2  female  24.0      1      2              220845   65.0000        S   \n304      3    male  24.0      0      0              349911    7.7750        S   \n305      2    male  57.0      0      0              244346   13.0000        S   \n306      3    male  21.0      0      0              364858    7.7500        Q   \n307      3    male   6.0      3      1              349909   21.0750        S   \n308      1    male  23.0      0      0               12749   93.5000        S   \n309      1  female  51.0      0      1            PC 17592   39.4000        S   \n310      3    male  13.0      0      2           C.A. 2673   20.2500        S   \n311      2    male  47.0      0      0          C.A. 30769   10.5000        S   \n312      3    male  29.0      3      1              315153   22.0250        S   \n313      1  female  18.0      1      0               13695   60.0000        S   \n314      3    male  24.0      0      0              371109    7.2500        Q   \n315      1  female  48.0      1      1               13567   79.2000        C   \n316      3    male  22.0      0      0              347065    7.7750        S   \n317      3    male  31.0      0      0               21332    7.7333        Q   \n318      1  female  30.0      0      0               36928  164.8667        S   \n319      2    male  38.0      1      0               28664   21.0000        S   \n320      1  female  22.0      0      1              112378   59.4000        C   \n321      1    male  17.0      0      0              113059   47.1000        S   \n322      1    male  43.0      1      0               17765   27.7208        C   \n323      2    male  20.0      0      0       SC/PARIS 2166   13.8625        C   \n324      2    male  23.0      1      0               28666   10.5000        S   \n325      1    male  50.0      1      1              113503  211.5000        C   \n326      3  female   3.0      1      1  SOTON/O.Q. 3101315   13.7750        S   \n327      1  female  37.0      1      0               19928   90.0000        Q   \n328      3  female  28.0      0      0              347086    7.7750        S   \n329      1  female  39.0      0      0            PC 17758  108.9000        C   \n330      3    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500        S   \n\n     Survived             Title  \n0           0             Kelly  \n1           1            Wilkes  \n2           0             Myles  \n3           0              Wirz  \n4           1          Hirvonen  \n5           0          Svensson  \n6           1          Connolly  \n7           0          Caldwell  \n8           1           Abrahim  \n9           0            Davies  \n10          0             Jones  \n11          1            Snyder  \n12          0            Howard  \n13          1           Chaffee  \n14          1         del Carlo  \n15          0             Keane  \n16          0             Assaf  \n17          1        Ilmakangas  \n18          1     \"Assaf Khalil  \n19          0        Rothschild  \n20          0             Olsen  \n21          0          Williams  \n22          1           Ryerson  \n23          0            Robins  \n24          1             Ostby  \n25          0             Daher  \n26          0             Brady  \n27          0             Louch  \n28          0          Jefferys  \n29          1              Dean  \n..        ...               ...  \n301         0             Foley  \n302         1           McNamee  \n303         1            Herman  \n304         0          Aronsson  \n305         0             Ashby  \n306         0           Canavan  \n307         0           Palsson  \n308         0             Payne  \n309         1             Lines  \n310         0            Abbott  \n311         0           Gilbert  \n312         0     Kink-Heilmann  \n313         1             Smith  \n314         0           Colbert  \n315         1  Frolicher-Stehli  \n316         0  Larsson-Rondberg  \n317         0            Conlon  \n318         1           Bonnell  \n319         0              Gale  \n320         1            Gibson  \n321         0            Carrau  \n322         0        Frauenthal  \n323         0          \"Nourney  \n324         0              Ware  \n325         0           Widener  \n326         1           Peacock  \n327         1           Minahan  \n328         1        Henriksson  \n329         1     Oliva y Ocana  \n330         0           Saether  \n\n[331 rows x 13 columns]"
                    }
                }
            ], 
            "execution_count": 72, 
            "source": "# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\n#display(dataset)\n#type(dataset)\ndataset.toPandas()\n\ntestdataSet = testdataSet.select(selectedcols)\ntestdataSet.toPandas()"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 25, 
            "source": "### Randomly split data into training and test sets. set seed for reproducibility\n# (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n# print trainingData.count()\n# print testData.count()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "root\n |-- label: double (nullable = true)\n |-- features: vector (nullable = true)\n |-- PassengerId: string (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Title: string (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = true)\n\nroot\n |-- label: double (nullable = true)\n |-- features: vector (nullable = true)\n |-- PassengerId: string (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Title: string (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = true)\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 82, 
                    "data": {
                        "text/plain": "712"
                    }
                }
            ], 
            "execution_count": 82, 
            "source": "from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(dataset)\n\npredictionssss = lrModel.transform(dataset)\npredictionssss.printSchema()\n\n# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testdataSet)\npredictions.printSchema()\n\npredictionssss.select(\"probability\").count()\n#predictions.select(\"probability\").count()\n#predictions.select(\"prediction\").count()"
        }, 
        {
            "metadata": {
                "scrolled": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-83-bf27c774b4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#predictions = predictions.select(\"label\", \"Age\", \"probability\", \"prediction\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \"\"\"\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4225.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 371.0 failed 10 times, most recent failure: Lost task 0.9 in stage 371.0 (TID 425, yp-spark-dal09-env5-0026, executor d6d48375-b2fd-4376-ac1a-f477005cfc4f): org.apache.spark.SparkException: Failed to execute user defined function(: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 904, y.size = 1803\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:753)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:752)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:886)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:682)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:954)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:953)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 904, y.size = 1803\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:753)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:752)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:886)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:682)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 16 more\n"
                    ], 
                    "ename": "Py4JJavaError", 
                    "evalue": "An error occurred while calling o4225.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 371.0 failed 10 times, most recent failure: Lost task 0.9 in stage 371.0 (TID 425, yp-spark-dal09-env5-0026, executor d6d48375-b2fd-4376-ac1a-f477005cfc4f): org.apache.spark.SparkException: Failed to execute user defined function(: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 904, y.size = 1803\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:753)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:752)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:886)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:682)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:954)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:953)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2745)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2742)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2742)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$26.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:290)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 904, y.size = 1803\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:753)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$27.apply(LogisticRegression.scala:752)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:886)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:682)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 16 more\n"
                }
            ], 
            "execution_count": 83, 
            "source": "# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\n\n\nselected = predictions.select(\"label\", \"Age\", \"probability\", \"prediction\")\nselected.toPandas()\nselected.show()\n#predictions = predictions.select(\"label\", \"Age\", \"probability\", \"prediction\")\n#predictions.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 25, 
                    "data": {
                        "text/plain": "0.8459883059931782"
                    }
                }
            ], 
            "execution_count": 25, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 26, 
            "source": "\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 27, 
            "source": "from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "numNodes =  15\ndepth =  3\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 28, 
            "source": "print \"numNodes = \", dtModel.numNodes\nprint \"depth = \", dtModel.depth"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 29, 
            "source": "# Make predictions on test data using the Transformer.transform() method.\npredictions = dtModel.transform(testData)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "+-----+----------+--------------------+----+\n|label|prediction|         probability| Age|\n+-----+----------+--------------------+----+\n|  0.0|       1.0|         [0.45,0.55]| 2.0|\n|  0.0|       1.0|         [0.45,0.55]| 8.0|\n|  0.0|       0.0|[0.88636363636363...|20.0|\n|  0.0|       0.0|[0.88636363636363...|28.0|\n|  0.0|       0.0|[0.88636363636363...|19.0|\n|  0.0|       0.0|[0.61111111111111...|45.0|\n|  0.0|       0.0|[0.88636363636363...|30.0|\n|  0.0|       0.0|[0.88636363636363...|48.0|\n|  0.0|       1.0|         [0.45,0.55]|11.0|\n|  0.0|       0.0|[0.88636363636363...|22.0|\n|  0.0|       1.0|         [0.45,0.55]| 2.0|\n|  0.0|       1.0|         [0.45,0.55]| 7.0|\n|  0.0|       0.0|       [0.875,0.125]| 3.0|\n|  0.0|       1.0|[0.40322580645161...|21.0|\n|  0.0|       0.0|[0.88636363636363...|36.0|\n|  0.0|       0.0|[0.88636363636363...|29.0|\n|  0.0|       0.0|[0.88636363636363...|17.0|\n|  0.0|       0.0|[0.88636363636363...|37.0|\n|  0.0|       0.0|       [0.875,0.125]| 2.0|\n|  0.0|       0.0|[0.61111111111111...|21.0|\n+-----+----------+--------------------+----+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 30, 
            "source": "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"Age\")\nselected.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 31, 
                    "data": {
                        "text/plain": "0.8239808348221538"
                    }
                }
            ], 
            "execution_count": 31, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 32, 
            "source": "from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 33, 
            "source": "predictions = rfModel.transform(testData)"
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "+-----+----------+--------------------+----+\n|label|prediction|         probability| Age|\n+-----+----------+--------------------+----+\n|  0.0|       0.0|[0.62972458353542...| 2.0|\n|  0.0|       0.0|[0.65428549005100...| 8.0|\n|  0.0|       0.0|[0.70355576696306...|20.0|\n|  0.0|       0.0|[0.68736980700028...|28.0|\n|  0.0|       0.0|[0.70355576696306...|19.0|\n|  0.0|       0.0|[0.57816693814403...|45.0|\n|  0.0|       0.0|[0.65910558916788...|30.0|\n|  0.0|       0.0|[0.70355576696306...|48.0|\n|  0.0|       0.0|[0.67105109354999...|11.0|\n|  0.0|       0.0|[0.70355576696306...|22.0|\n|  0.0|       0.0|[0.66164945976849...| 2.0|\n|  0.0|       0.0|[0.68621036628407...| 7.0|\n|  0.0|       0.0|[0.61090255946120...| 3.0|\n|  0.0|       0.0|[0.63226818574442...|21.0|\n|  0.0|       0.0|[0.70355576696306...|36.0|\n|  0.0|       0.0|[0.68736980700028...|29.0|\n|  0.0|       0.0|[0.70355576696306...|17.0|\n|  0.0|       0.0|[0.62660091575481...|37.0|\n|  0.0|       0.0|[0.62892879089358...| 2.0|\n|  0.0|       0.0|[0.59435289810682...|21.0|\n+-----+----------+--------------------+----+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 34, 
            "source": "# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"Age\")\nselected.show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 35, 
                    "data": {
                        "text/plain": "0.8243462725353256"
                    }
                }
            ], 
            "execution_count": 35, 
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-36-00846046f1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o933.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 10 times, most recent failure: Lost task 0.9 in stage 107.0 (TID 113, yp-spark-dal09-env5-0026, executor c1ebe4b5-e896-4500-97eb-89bbcaaf42ca): java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 1\n\tat org.apache.spark.ml.classification.LabelConverter$.encodeLabeledPoint(MultilayerPerceptronClassifier.scala:121)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1076)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1091)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1128)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:964)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:895)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:701)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:337)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1176)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:817)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:260)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:145)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 1\n\tat org.apache.spark.ml.classification.LabelConverter$.encodeLabeledPoint(MultilayerPerceptronClassifier.scala:121)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1076)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1091)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1128)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:964)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:895)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:701)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:337)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
                    ], 
                    "ename": "Py4JJavaError", 
                    "evalue": "An error occurred while calling o933.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 10 times, most recent failure: Lost task 0.9 in stage 107.0 (TID 113, yp-spark-dal09-env5-0026, executor c1ebe4b5-e896-4500-97eb-89bbcaaf42ca): java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 1\n\tat org.apache.spark.ml.classification.LabelConverter$.encodeLabeledPoint(MultilayerPerceptronClassifier.scala:121)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1076)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1091)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1128)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:964)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:895)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:701)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:337)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1971)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1176)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimize(LBFGS.scala:142)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:817)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:260)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:145)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 1\n\tat org.apache.spark.ml.classification.LabelConverter$.encodeLabeledPoint(MultilayerPerceptronClassifier.scala:121)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$anonfun$3.apply(MultilayerPerceptronClassifier.scala:238)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1076)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1091)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1128)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1132)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:964)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:895)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:955)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:701)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:337)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
                }
            ], 
            "execution_count": 36, 
            "source": "from pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlayers = [4, 5, 4, 1]\n\n# create the trainer and set its parameters\ntrainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n\n# train the model\nmodel = trainer.fit(dataset)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": ""
        }
    ], 
    "nbformat_minor": 1, 
    "nbformat": 4
}